\chapter{Preliminaries on Graphs and Probabilistic Finite State Automata}\label{cap:2}

\tikzset{
  treenode/.style = {shape=circle,
                     draw, align=center,
                     top color=white},
  root/.style     = {treenode, font=\ttfamily\normalsize},
  env/.style      = {treenode, font=\ttfamily\normalsize},
  leaf/.style    = {treenode,font=\ttfamily\normalsize, bottom color=red}
}

{\lettrine[loversize=0.25,findent=0.2em,nindent=0em]{I}{n} this chapter we revise concepts from graphs and PFSA \cite{brianmarcus}\cite{vidal.05} that will be required in the subsequent chapters. The concept of graph minimization is presented and two mainstream algorithms to achieve this are described, Moore and Hopcroft. Finally, two well known algorithms to model dynamic systems with PFSA are presented, D-Markov and CRISSiS.

\section{Sequences of Discrete Symbols}
This section provides tools to describe sequences of discrete symbols. A finite sequence $u$ of symbols from an alphabet $\Sigma$ is called a word and its length is denoted by $|\textit{u}|$. The empty word $\varepsilon$ is defined as the sequence with length 0. The set of all possible words of length $n$ symbols from $\Sigma$ is $\Sigma^n$ and the set of all sequences of symbols from $\Sigma$ with all possible lengths, including the empty sequence sequence $\varepsilon$, is $\Sigma^*$. 

Two words \textit{u} and \textit{v} $\in \Sigma^*$ can be concatenated to form a sequence \textit{uv}. For example, using a binary alphabet, $\Sigma = \{0,1\}$, the concatenation of \textit{u} = 1010 and \textit{v} = 111 is \textit{uv} = 1010111. Note that $|\textit{uv}| = |\textit{u}| + |\textit{v}|$. Concatenation is associative, which means \textit{u}(\textit{vw}) = (\textit{uv})\textit{w} = \textit{uvw}, but it is not commutative, as \textit{uv} is not necessarily equal to \textit{vu}. The empty word $\varepsilon$ is a neutral element for concatenation, that is, $\varepsilon u = u\varepsilon = u$. This means that $\Sigma^*$ with the operation of concatenation is a Monoid, as it is a set with an associative operation with an identity element.

A word $v \in \Sigma^*$ is called a suffix of a word $w \in \Sigma^*$ ( $|\textit{w}| > |\textit{v}|$) if $w$ can be written as a concatenation $uv$, where $u \in \Sigma^*$. In this same sense, the sequence $u$ is called a prefix of $w$. 

\section{Graphs}

\begin{definition}[\textbf{Graph}]\label{def:graph}
A graph \textit{G} over the alphabet $\Sigma$ consists of a triple ($Q, \Sigma,\delta$):
\begin{itemize}
	\item $Q$ is a finite set of states with cardinality $|Q|$;
    \item $\Sigma$ is a finite alphabet with cardinality $|\Sigma|$;
    \item $\delta$ is the state transition function $Q\times\Sigma \rightarrow Q$;
\end{itemize}
\end{definition}

\noindent Each state $q \in Q$ can be represented as a dot or circle and if $\exists\, \delta(q, \sigma) = q^{\prime}$, for $q, q^{\prime} \in Q$ and $\sigma \in \Sigma$, this transition can be represented with a directed arrow from state $q$ to state $q^{\prime}$ labeled with the symbol $\sigma$. This realization of the transition function is called the outgoing edge from $q$ to $q^{\prime}$ with symbol $\sigma$. Figure \ref{fig:graph} shows an example of a three-state graph over a binary alphabet from where it possible to see there is an outgoing edge from state $A$ to state $B$ with the symbol $1$, thus $\delta(A, 1) = B$.

It is possible to extend the transition function so it accepts words and not just symbols. Given $\omega \in \Sigma^n$, where $\omega = \sigma_1\sigma_2\ldots\sigma_n$ with $\sigma_m \in \Sigma$, for $m = 1\ldots n$, and given states $q_0, q_1, \ldots, q_n \in Q$, we define the function $\delta^*(q_0, \omega) = q_n$ if $\delta(q_0, \sigma_1) = q_1, \delta(q_1,\sigma_2) = q_2, \ldots, \delta(q_{n-1},\sigma_n) = q_n$. If $\exists\, \omega \in \Sigma^*$ such that for two states $q_1, q_2 \in Q$, $\delta^*(q_1,\omega) = q_2$, it is said there is a path between $q_1$ and $q_2$ and that $\omega$ is generated by $G$. In Figure \ref{fig:graph} the path starting at state $A$ and going through $A$, $A$, $B$, $C$, $B$, $A$ generates the word $\omega	$ = 001110.

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =3 cm and 3cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (C)
{$C$};
\node[state] (A) [above left=of C] {$A$};
\node[state] (B) [above right =of C] {$B$};
\path (A) edge [loop left] node[left] {$0$} (A);
\path (C) edge [bend left =] node[below =0.15 cm] {$0$} (A);
%\path (A) edge [bend right = -15] node[below =0.15 cm] {$1/2$} (C);
\path (A) edge [bend left =25] node[above] {$1$} (B);
\path (B) edge [bend left =15] node[below =0.15 cm] {$0$} (A);
\path (C) edge [bend left =15] node[below =0.15 cm] {$1$} (B);
\path (B) edge [bend right = -25] node[below =0.15 cm] {$1$} (C);
\end{tikzpicture}
\caption{A three-state graph with $Q$ = \{A, B, C\} and $\Sigma = \{0, 1\}$.\label{fig:graph}}
\end{figure}


\begin{definition}[\textbf{Follower Set}]\label{def:followerset}
The follower set of a state $q \in Q$ is defined as the set of all possible words generated by paths that start at $q$ and end in a state of $Q$:
\end{definition}

\[ F(q) = \{\omega \in \Sigma^* | \delta^*(q, \omega) \in Q\}. \]


\begin{definition}[\textbf{Language of a Graph}]\label{def:language}
The language $\mathcal{L}$ of a graph \textit{G} is the the set of follower sets for each state \textit{q} $\in Q$:
\end{definition}

\[ \mathcal{L} = \{F(\textit{q}), \forall \textit{q} \in Q\}. \]

\noindent A word $\omega \in \Sigma^*$ is called a synchronization word of $G$ if starting from any state $q \in Q$ that generates $\omega$ the same state in $Q$ is reached. That is, if $\omega$ is a synchronization word, $\delta^*(q, \omega) = q^{\prime}$, for any $q\in Q$ that generates $\omega$. In the graph of Figure \ref{fig:graph}, $0$ is a synchronization word that synchronizes to state $A$.

\section{Graph Minimization}

Suppose that two graphs $G_1 = \{Q_1, \Sigma, \delta_1\}$ and $G_2 = \{Q_2, \Sigma, \delta_2\}$ with $|Q_1|\neq |Q_2|$ and are capable of generating the same language. It is desirable to use a graph with fewer states as it can be represented with a lower memory requirement. This is called a minimal graph. 

\begin{definition}[\textbf{Minimal Graph}]\label{def:mingraph}
For a given language $\mathcal{L}$ there is a minimal graph $G_{min} = \{Q,\Sigma ,\delta \}$ capable of generating it. The minimal graph is the one for which each state $q \in Q$ has a distinct follower set.
\end{definition}

If a graph has two distinct states $q_1$ and $q_2$ with the same follower set, a new graph that generates the same language can be obtained by merging these states, i.e. replace $q_1$ and $q_2$ by a new state $q$ such that if any state $q\prime$ has an outgoing edge to either $q_1$ or $q_2$ ($\delta(q\prime,\sigma) = q_1$ or $q_2$ for some $\sigma\in\Sigma$) it will now be an outgoing edge to $q$ ($\delta(q\prime,\sigma) = q$) and copying all of the outgoing edges of both $q_1$ and $q_2$ to $q$. When all the states have distinct follower sets, none of them can be excluded without affecting the generated language.

From Definition \ref{def:mingraph} it is possible to define an equivalence relation called the Nerode equivalence:

\[
p, q \in Q, p \equiv q \Leftrightarrow F(p) = F(q).
\]

\noindent A graph is considered minimal if and only if its Nerode equivalence is the identity. The problem of minimizing a graph is that of computing the Nerode equivalence. The minimal graph accepts the same language as the original graph.


Given a graph $G$ there are two main algorithms used to obtain a minimal graph from it: Moore and Hopcroft \cite{berstel.10}. Both will be described in this section, but some definitions are due before getting into the algorithms.

\begin{definition}[\textbf{Partitions and Equivalence Relations}]\label{def:partition}
Given a set $E$, a partition of $E$ is a family $\mathcal{P}$ of nonempty, pairwise disjoint subsets of $E$ such that $\bigcup_{P\in\mathcal{P}}P = E $. The index of the partition is its number of elements. The partition $\mathcal{P}$ defines an equivalence relation on $E$ and the set of all equivalence classes of an equivalence relation in $E$ defines a partition of the set.
\end{definition}

When a subset \textit{F} of \textit{E} is the union of classes of $\mathcal{P}$ it said that \textit{F} is saturated by $\mathcal{P}$. Given $\mathcal{Q}$, another partition of \textit{E}, it said to be a \textit{refinement} of $\mathcal{P}$ (or that $\mathcal{P}$ is coarser than $\mathcal{Q}$) if every class of $\mathcal{Q}$ is contained by some class of $\mathcal{P}$ and it is written as $\mathcal{Q} \leq \mathcal{P}$. The index of $\mathcal{Q}$ is greater than the index of $\mathcal{P}$.

Given partitions $\mathcal{P}$ and $\mathcal{Q}$ of \textit{E}, $\mathcal{U} = \mathcal{P}\wedge\mathcal{Q}$ denotes the coarsest partition which refines $\mathcal{P}$ and $\mathcal{Q}$. The elements of $\mathcal{U}$ are non-empty sets $P\,\cap\,Q$, such that $P \in\mathcal{P}$ and 	$Q \in\mathcal{Q}$. The notation is extended for multiple sets as $\mathcal{U} = \mathcal{P}_1 \wedge \mathcal{P}_2 \wedge \ldots \wedge \mathcal{P}_n = \bigwedge\limits_{j=1}^n\mathcal{P}_j$. When $n=0$,  $\mathcal{P}$ is the universal partition comprised of just \textit{E} and it is the neutral element for the $\wedge$-operation.

Given $F\subseteq E$, a partition $\mathcal{P}$ of \textit{E} induces a partition $\mathcal{P}'$  
of \textit{F} by intersection. $\mathcal{P}'$ is composed by the sets $P\cap F$ with $P\subseteq \mathcal{P}$. If $\mathcal{P}$ and $\mathcal{Q}$ are partitions of \textit{E} and $\mathcal{Q} \leq \mathcal{P}$, the restrictions $\mathcal{P}'$ and $\mathcal{Q}'$ to \textit{F} maintain $\mathcal{Q}' \leq \mathcal{P}'$.

%Given partitions $\mathcal{P}$ and $\mathcal{P}\prime$ of disjoint sets $E$ and $E\prime$, the partition of set $E \cup E\prime$ whose restriction to $E$ and $E\prime$ are $\mathcal{P}$ and $\mathcal{P'}$ is denoted by $\mathcal{P}\vee\mathcal{P}'$. It is possible to write $\mathcal{P} = \vee_{P\vee\mathcal{P}}\{P\}$.


Given a set of states $P \subset Q$ and a symbol $\sigma \in \Sigma$, let $\sigma^{-1}\textit{P}$ denote the set of states $q\in Q$ such that $\delta(q, \sigma) \in P$. Consider $P, R\subset Q$ and $\sigma \in \Sigma$, the partition of $R$

\[
(P, \sigma)|R
\]

\noindent is the partition composed of two non-empty subsets:

\begin{equation}\label{eq:splitintersec}
R\cap\sigma^{-1}P = \{r \in R | \delta(r,\sigma) \in P\}
\end{equation}

\noindent and

\begin{equation}\label{eq:splitquot}
R\backslash\sigma^{-1}P = \{r \in R | \delta(r,\sigma) \notin P\}.
\end{equation}

\noindent The pair $(P, \sigma)$ is called a splitter. Observe that $(P, \sigma)|R = R$ if either $\delta(r,\sigma) \subset P$
or $\delta(r,\sigma)\,\cap\,P = \emptyset,\,\forall r \in R$ and $(P,\sigma)|R$ is composed of two classes if both $\delta(r,\sigma)\cap P  \neq \emptyset$ and
$\delta(r,\sigma)\cap P^c  \neq \emptyset, \,\forall r \in R$  or equivalently if $\delta(r,\sigma) \not\subset P $   and  $\delta(r,\sigma)\not\subset P^c, \, \forall r \in R $ . If $(P, \sigma)|R$ contains two classes, then we say that $(P, \sigma)$ splits $R$. This notation can also be extended to sequences, using a sequence $\omega \in \Sigma^*$ instead of the symbol $\sigma \in \Sigma$.

\begin{proposition}\label{prop:nerequiv}
The partition corresponding to the Nerode equivalence is the coarsest partition $\mathcal{P}$ such that no splitter $(P,\sigma)$, with $P \in \mathcal{P}$ and $\sigma \in \Sigma$, splits a class in $\mathcal{P}$, such that $(P,\sigma)|R = R$ for all $P, R \in \mathcal{P}$ and $\sigma \in \Sigma$.
\end{proposition}

%\begin{lemma}\label{lemm:hopcrof}
%Let P be a set of states and $\mathcal{P} = {P_1, P_2}$ a partition of P. For any symbol $\sigma$ and for any set of states R, one has:
%\end{lemma}
 
%
%\[
%(P,\sigma)|R \wedge (P_1, \sigma)|R = (P, \sigma)|R \wedge (P_2, \sigma)|R = (P_1,\sigma)|R \wedge (P_2,\sigma)|R,
%\]
%
%\textit{and consequently}
%
%\[
%(P,\sigma)|R \geqslant (P_1,\sigma)|R \wedge (P_2,\sigma)|R,
%\]
%\[
%(P_1,\sigma)|R \geqslant (P,\sigma)|R \wedge (P_2,\sigma)|R.
%\]
\subsection{Moore Algorithm}

The first minimization algorithm is the Moore algorithm \cite{moore1956gedanken}. It is based on the idea of taking an initial partition with a very wide criteria and then refining it until the Nerode equivalence classes are obtained. The outline of the algorithm is shown in Algorithm \ref{alg:moore}.

\begin{algorithm} [b]
  \caption{Moore(\textit{G})\label{alg:moore}}
    \begin{algorithmic}[1]
      \State $\mathcal{P} \leftarrow InitialPartition(G)$
      \Repeat
      	\State $\mathcal{P}' \leftarrow \mathcal{P}$
      	\ForAll{$\sigma \in \Sigma$}
      		\State $\mathcal{P}_{\sigma} \leftarrow \bigwedge_{P\in \mathcal{P}}(P,\sigma)|Q$
      	\EndFor
      	\State $\mathcal{P} \leftarrow \mathcal{P}\wedge\bigwedge_{\sigma\in\Sigma}\mathcal{P}_{\sigma}$
      \Until{$\mathcal{P} = \mathcal{P}'$}
    \end{algorithmic}
  \end{algorithm}
 
Given a graph \textit{G = (Q, $\Sigma, \delta$)} and the initial partition $\mathcal{P} = \{P_1,\ldots, P_n\}$, the set $L_q^{(h)}$ is defined as:

\[
L_q^{(h)}(G) = \{w \in \Sigma^* | |w| \leq h, \delta^*(q,w) \in P_j\},
\]

\noindent where $\mathcal{P}_j \in \mathcal{P}$  is comprised of all words up to length $h$ that can be generated starting from a certain $q \in Q$ and reaching a state in the equivalence class $\mathcal{P}_j$. The Moore equivalence of order $h$ (denoted by $\equiv_h$) is defined by:

\[
p \equiv_h q \Leftrightarrow L_p^{(h)}(G) = L_q^{(h)}(G).
\]

\noindent This equivalence relation states that two states are equivalent if they generate the same words of length up to $h$ that reach a state in $\mathcal{P}_j$. The depth of the Moore algorithm on a graph $G$ is the integer $h$ such that the Moore equivalence $\equiv_h$ becomes equal to the Nerode equivalence $\equiv$ and it is dependent only on the language of the graph. The depth is the smallest $h$ such that $\equiv_h$ equals $\equiv_{h+1}$, which leads to an algorithm that computes successive Moore equivalences until it finds two consecutive equivalences that are equal, making it halt.

\begin{proposition}\label{prop:makemooreeasy}
For two states $p, q \in Q$ and $h \geq 0$, one has

\end{proposition}

\begin{equation}\label{eq:mooreequiv}
p \equiv_{h+1} q \Longleftrightarrow p \equiv_{h} q\; \text{and} \;\delta(p,\sigma) \equiv_h \delta(q,\sigma), \; \forall \sigma \in \Sigma.
\end{equation}

\noindent Using this formulation and defining $\mathcal{M}_h$ as the partition defined by the Moore equivalence of depth $h$, the following equations hold:

\begin{proposition}\label{prop:moorecomp}
For \textit{h} $\geq$ 0, one has

\end{proposition}

\begin{equation}\label{eq:mooreequiv}
\mathcal{M}_{h+1} = \mathcal{M}_h \wedge \bigwedge_{\sigma\in\Sigma} \bigwedge_{P\in\mathcal{M}_h}(P,\sigma)|Q.
\end{equation}

\noindent This computation means that for each symbol $\sigma\in\Sigma$ and for each equivalence class $P\in\mathcal{M}_h$ (where $mathcal{M}_h$ is the partition of the previous iteration) a splitter $(P,\sigma)$ is created and applied to the original set of states $Q$. This will create partitions that show which states of $Q$ reach states in $P$ with symbol $\sigma$ and which do not. Then the coarsest partition $\bigwedge_{P\in\mathcal{M}_h}(P,\sigma)|Q$ is taken, which will separate the equivalence classes of states of $Q$ that reach different equivalence classes of $\mathcal{M}_h$ with a given symbol $\sigma$. After this, the coarsest partition $\bigwedge_{\sigma\in\Sigma} \bigwedge_{P\in\mathcal{M}_h}(P,\sigma)|Q$ between these equivalence classes are taken, separating $Q$ in classes that reach classes in $\mathcal{M}_h$ with each different symbol of $\Sigma$. This effectively computes the $\delta(p,\sigma) \equiv_h \delta(q,\sigma), \; \forall \sigma \in \Sigma$ part of (\ref{eq:mooreequiv}). To finish (\ref{eq:mooreequiv}) the coarsest partition of this last step and of $\mathcal{M}_h$ is taken, resulting in the effective computation of an increment in the Moore equivalence classes.

 This previous computation is performed in Algorithm \ref{alg:moore} in which the loop refines the current partition until no change occurs between $\mathcal{M}_h$ and $\mathcal{M}_{h+1}$, which means the Nerode equivalence is reached. As it will be explored in this work, the initial partition can be created with different criteria. For a graph, it is done by grouping together states in $Q$ which have outgoing edges with the same labels, but another criterion is used in the probabilistic case (Section \ref{subsec:inipart}).

 This previous computation is performed in Algorithm \ref{alg:moore} in which the loop refines the current partition. As will be explored in this work, the initial partition can be created with different criteria. For a graph, it is done by grouping together states in $Q$ which have outgoing edges with the same labels, but another criterion is used in the probabilistic case (see Section \ref{subsec:inipart}).

Moore algorithm of the refinement of $k$ partition of a set with $n$ elements can be done in time $O(kn^2)$. Each loop is processed in time $O(kn)$, so the total time is $O(m kn)$, where $m$ is the total number of refinement steps needed to compute the Nerode equivalence.

\subsubsection{An Example}

To illustrate how the Moore algorithm works, this example will apply it to the graph of Figure \ref{fig:moore_ex}, which has $Q = \{A, B, C, D, E\}$, $\Sigma = \{0, 1\}$ and it is not minimal. The first step is to create the initial partition based on the criteria of grouping states together in equivalence classes if they have the same outgoing edges with the same labels. In Figure \ref{fig:moore_ex}, states $A$ and $D$ have outgoing edges labeled with 0 and 1 while $B, C$ and $E$ have only an outgoing edge labeled with 0. Thus, the initial partition has two equivalence classes, $\mathcal{P} = \{\{A,D\}$,  $\{B,C,E\}\}$.

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =2 cm and 2cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (A)  {$A$};
\node[state] (B) [below right =of A] {$B$};
\node[state] (C) [below  =of B] {$C$};
\node[state] (E) [below left =of A] {$E$};
\node[state] (D) [below =of E] {$D$};
\path (A) edge [bend right=15] node[right] {$0$} (C);
\path (A) edge [bend left=15] node[left] {$1$} (E);
\path (B) edge [bend right=15] node[right] {$0$} (A);
\path (E) edge [bend left=15] node[left] {$0$} (A);
\path (C) edge [bend right=15] node[left] {$0$} (B);
\path (D) edge [bend right=15] node[below] {$0$} (C);
\path (D) edge [bend left=15] node[left] {$1$} (E);
\end{tikzpicture}
\caption{An example of a graph that is not minimal.\label{fig:moore_ex}}
\end{figure}

Applying the Moore algorithm, $\mathcal{P}^{\prime}$ will store the current state of $\mathcal{P}$. First, consider $\sigma = 0$. To create the equivalence class $\mathcal{P}_0$, we consider the splitters $(\{A,D\},0)$ and $(\{B,C,E\},0)$ applied to $Q$. First, take $(\{A,D\},0)|Q$. From (\ref{eq:splitintersec}) we have:

\[
Q\cap 0^{-1}\{A,D\} = \{B,E\}
\]

\noindent and from (\ref{eq:splitquot}): 

\[
Q\backslash 0^{-1}\{A,D\} = \{A, C, D\}.
\]

\noindent The same process is repeated for the splitter $(\{B,C,E\},0)$. From (\ref{eq:splitintersec}):

\[
Q\cap 0^{-1}\{B,C,E\} = \{A,C,D\}
\]

\noindent and from  (\ref{eq:splitquot}):

\[
Q\backslash 0^{-1}\{B,C,E\} = \{B,E\}.
\]

\noindent $\mathcal{P}_0$ is then the coarsest partition between $(\{A,D\},0)|Q = \{\{B,E\},\{A,C,D\}\}$ and \linebreak $(\{B,C,E\},0)|Q = \{\{A,C,D\},\{B,E\}\}$. Thus $\mathcal{P}_0 = \{\{A,C,D\},\{B,E\}\}$.

This process is repeated to obtain $\mathcal{P}_1$. $(\{A,D\},1)|Q$: from (\ref{eq:splitintersec}):

\[
Q\cap 1^{-1}\{A,D\} = \emptyset
\]

\noindent and from (\ref{eq:splitquot}):

\[
Q\backslash 1^{-1}\{A,D\} = \{A, B, C, D, E\}
\]

\noindent and for $(\{B,C,E\},1)|Q$: 

\[
Q\cap 1^{-1}\{B,C,E\} = \{A,D\}
\]

\noindent and 

\[
Q\backslash 0^{-1}\{A,D\} = \{B, C, E\}.
\]

\noindent The coarsest partition between $\{\{A,B,C,D,E\}\}$ and \linebreak $\{\{A,D\},\{B,C,E\}\}$ is $\{\{A,D\},\{B,C,E\}\} = \mathcal{P}_1$.

The next step is to take the coarsest partition between $\mathcal{P}_0$ and $\mathcal{P}_1$, which is $\{\{A,D\},\{C\},\{B,E\}\}$. Then the coarsest partition between this result and the current $\mathcal{P}$ is taken, which leaves it unchanged. This result is then stored as the new partition $\mathcal{P}$ and it is shown in Figure \ref{fig:moore_fin}.


%\begin{figure}
%\centering
%\begin {tikzpicture}[-latex ,auto ,node distance =3 cm and 3cm ,on grid ,
%semithick ,
%state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
%\node[state] (A)  {$A,D$};
%\node[state] (B) [right =of A] {$B,C,E$};
%\path (A) edge [bend right=15] node[below] {$0$} (B);
%\path (A) edge [bend left=15] node[above] {$1$} (B);
%\path (B) edge [bend right=45] node[above] {$0$/$B \rightarrow A, E\rightarrow A$} (A);
%\path (B) edge [loop right=15] node[right] {$0$/$C \rightarrow B$} (B);
%\end{tikzpicture}
%\caption{Initial partition of the graph in Figure \ref{fig:moore_ex}.\label{fig:moore_ini}}
%\end{figure}

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =3 cm and 3cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (A)  {$A,D$};
\node[state] (C) [below right =of A] {$C$};
\node[state] (B) [above right =of C] {$B,E$};
\path (A) edge [bend right=15] node[below] {$0$} (C);
\path (A) edge [bend right=15] node[above] {$1$} (B);
\path (B) edge [bend right=15] node[above] {$0$} (A);
\path (C) edge [bend right=15] node[right] {$0$} (B);
\end{tikzpicture}
\caption{Application of Moore algorithm to the initial partition.\label{fig:moore_fin}}
\end{figure}

As the current $\mathcal{P}$ is different from the one stored in $\mathcal{P}^{\prime}$, a new iteration has to be performed. Now, $\mathcal{P}$ overwrites the old $\mathcal{P}^{\prime}$ and we have to compute $\mathcal{P}_0$ and $\mathcal{P}_1$.

First, the result of the splitters for 0 are $(\{A,D\},0)|Q = \{\{B,E\},\{A,C,D\}\}$, $(\{C\},0)|Q = \{\{A,D\},\{B,C,E\}\}$ and $(\{B,E\},0)|Q = \{\{C\},\{A,B,D,E\}\}$ which results in \linebreak $\mathcal{P}_0 = \{\{B,E\}, \{A,D\}, \{C\}\}$. Similarly, $(\{A,D\},1)|Q = \{\{A,B,C,D,E\}\}$, $(\{C\},1)|Q = \{\{A,B,C,D,E\}\}$ and $(\{B,E\},1)|Q = \{\{A,D\},\{B,C,E\}\}$ and results in \linebreak $\mathcal{P}_1 = \{\{A,D\},\{B,C,E\}\}$. The coarsest partition between $\mathcal{P}_0$ and $\mathcal{P}_1$ is $\{\{A,D\},\{C\},\{B,E\}\}$ which remains unchanged when its coarsest partition is taken with $\mathcal{P}$. This result is then stored in $\mathcal{P}$ and it is equal to $\mathcal{P}^{\prime}$, which means the algorithm has converged and the minimal graph is shown in Figure \ref{fig:moore_fin}.

\subsection{Hopcroft's Algorithm}

\begin{algorithm} 
  \caption{Hopcroft(\textit{G})\label{alg:hop}}
    \begin{algorithmic}[1]
      \State $\mathcal{P} \leftarrow InitialPartition(G)$
      \State $\mathcal{W} \leftarrow \emptyset$
      \ForAll{$\sigma \in \Sigma$}
      	\State Append(($\min$(\textit{F}, $F^c$,$\sigma$),$\mathcal{W}$)
      	\While{$\mathcal{W} \neq \emptyset$}
      		\State (\textit{W},$\sigma$) $\leftarrow$ TakeSome($\mathcal{W}$)
      		\For{each \textit{P}$\in \mathcal{P}$ which is split by $(W,\sigma)$}
				\State $P^{\prime}, P^{\prime\prime} \leftarrow (W,\sigma)|P$      		
				Replace \textit{P} by \textit{P'} and \textit{P"} in $\mathcal{P}$
				\ForAll{$\tau \in \Sigma$}
					\If{$(P,\tau)\in\mathcal{W}$}
						\State Replace $(P,\tau)$ by $(P^{\prime},\tau)$ and $(P^{\prime\prime},\tau)$ in $\mathcal{W}$
					\Else
						\State Append(($\min(P^{\prime}, P^{\prime\prime},\tau),\mathcal{W}$)				
					\EndIf				
				\EndFor 
      		\EndFor
      	\EndWhile
      \EndFor
    \end{algorithmic}
  \end{algorithm}

The notation min($P,P^{\prime}$) indicates the set of smaller size of the two sets $P$ and $P^{\prime}$ or any of them when both have the same size. Hopcroft's algorithm computes the coarsest partition that saturates the set $F$ of final states. The algorithm keeps a current partition $\mathcal{P} = \{P_1, \ldots, P_n\}$ and a current set $\mathcal{W}$ of splitters (i.e. pairs ($W, \sigma$) that remain to be processed where $W$ is a class of $\mathcal{P}$ and $\sigma$ is a letter) which is called the \textit{waiting set}. $\mathcal{P}$ is initialized with the initial partition following the same criteria as described in Moore's algorithm. The waiting set is initialized with all the pairs (min($F, F^c$), $\sigma$) for $\sigma\in\Sigma$.

For each iteration of the loop, one splitter ($W,\sigma$) is taken from the waiting set. It then checks whether ($W, \sigma$) splits each class of $P$ of $\mathcal{P}$. If it does not split, nothing is done, but if it does then $P^{\prime}$ and $P^{\prime\prime}$ (which are the result of splitting \textit{P} by ($W,\sigma$) replace $P$ in $\mathcal{P}$. Next, for each letter $\tau\in\Sigma$, if the pair ($P,\tau$) is present in $\mathcal{W}$ is replaced by the two pairs ($P^{\prime},\tau$) and ($P^{\prime\prime}\tau$). Otherwise, only (\textit{min}$(P^{\prime},P^{\prime\prime}),\tau$) is added to $\mathcal{W}$.

The previous computation is performed until $\mathcal{W}$ is empty. It is proven that the final partition of the algorithm is the same as the one given by the Nerode equivalence. No specific order of pairs ($W, \sigma$) is described, which gives rise to different implementations in how the pairs are taken from the waiting set but all of them produce the right partition of states. Hopcroft proved that the running time of any execution of his algorithm is bounded by \textit{O(|$\Sigma| n\log n$)}.

\section{Probabilistic Finite State Automata}

\begin{definition}[\textbf{Probabilistic Finite State Automata}]\label{definition:pfsa}
A PFSA is defined as a graph $G$ and a probability function $\pi$ associated to each of its outgoing edges, i.e. $(G, \pi)$. The function $\pi: Q\times\Sigma \rightarrow [0,1]$ such that for a state $q \in Q$, $\sum\limits_{\sigma\in\Sigma}\pi(q,\sigma) = 1$, defines a probability distribution associated with each state of $G$.
\end{definition} 

\begin{definition}[\textbf{Morph}]\label{definition:morph}
Given a state $q \in Q$, the probability distribution $\mathcal{V}(q) = \{ \pi(q, \sigma); \forall \sigma \in \Sigma\}$ associated with $q$ is called its morph.  
\end{definition}

A PFSA is drawn with its graph with each outgoing edge labeled with a symbol and the probability $\pi(q,\sigma)$ associated with that transition. An example of a PFSA is shown in Figure \ref{fig:pfsa}. for which $Q$ = $\{A, B, C\}$, $\Sigma = \{0, 1\}$. It is the same graph from Figure \ref{fig:graph} with probabilities associated to its edges to create a PFSA.

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =3 cm and 3cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (C)
{$C$};
\node[state] (A) [above left=of C] {$A$};
\node[state] (B) [above right =of C] {$B$};
\path (A) edge [loop left] node[left] {$0|0.25$} (A);
\path (C) edge [bend left = 15] node[left =0.15 cm] {$0|0.5$} (A);
%\path (A) edge [bend right = -15] node[below =0.15 cm] {$1/2$} (C);
\path (A) edge [bend left =15] node[above] {$1|0.75$} (B);
\path (B) edge [bend left =15] node[above =0.15 cm] {$0|0.2$} (A);
\path (C) edge [bend left =15] node[left =0.15 cm] {$1|0.5$} (B);
\path (B) edge [bend right = -25] node[right =0.15 cm] {$1|0.8$} (C);
\end{tikzpicture}
\caption{A PFSA with the same graph of Figure \ref{fig:graph}.\label{fig:pfsa}}
\end{figure}

Given a PFSA $\{G, \pi\}$, there is a probability associated with each word $\omega \in \Sigma^*$ that can be generated from each state of $G$. From Figure \ref{fig:pfsa}, starting at the state $A$, it is possible to generate the word $\omega = 1011001$ (as $\delta^*(A, \omega) = B$) by taking a path going to states $B,A,B,C,A,A$ and $B$ and concatenating the labels of the path from each of these transitions. By multiplying the probabilities of these edges, it is seen that  = $\Pr (\omega |A) = 0.75\times0.2\times0.75\times0.8\times0.5\times0.25\times0.75 = 0.0084375$.

It is useful do adapt the concept of synchronization word to the context of PFSA as defined in \citep{asok.11}.

\begin{definition}[\textbf{PFSA Synchronization Word}]\label{definition:synchword}
The word $w$ is a synchronization word if, $\forall \textit{u} \in \Sigma^*$ and $\forall \textit{v} \in \Sigma^*$:

\begin{equation}
\Pr(\textit{u}|\textit{w}) = \Pr(\textit{u}|\textit{vw}).
\label{eq:synchword}
\end{equation}
\end{definition}

\noindent Definition \ref{definition:synchword} means that the probability of obtaining any sequence after the synchronization word does not depend on whatever came before \textit{w}. The main problem with this definition is the fact that is not possible to check (\ref{eq:synchword}) for all \textit{u} $\in \Sigma^*$ and for all \textit{v} $\in \Sigma^*$ as there are an infinite number of sequences.The solution is to use (\ref{eq:practsynchword}):

\begin{equation}
\Pr(\textit{u|w}) = \Pr(\textit{u|vw}), \forall u \in \cup_{i=1}^{L_1}\Sigma^i, \forall v \in \cup_{j=1}^{L_2}\Sigma^j  
\label{eq:practsynchword}
\end{equation}

%  A solution uses the \textit{d}-th order derived frequency, which is the probability using \textit{u} and \textit{v} from $\Sigma^d$, \textit{d} $\in \mathbb{Z}$, instead of taking them from $\Sigma^*$. Calling $\Pr_d(\omega)$ the d-th order derived frequency of $\omega$, a statistical test (such as the Chi-Squared or Kolmogorov-Smirnov) with significance level $\alpha$ has to be performed with the following null hypothesis for \textit{w} being a synchronization word:


\noindent where $L_1$ and $L_2$ are precision parameters. This means that all words $u$ of length up to $L_1$ are checked as past previous to $w$ and all words $v$ up to length $L_2$ are checked as continuations. This limits the number of tests to be performed, as the tests have to check $|\Sigma|^{L_1 + 1}$ previous words and $|\Sigma|^{L_2+1}$ continuation words.

%This means that the statistical test compares the probabilities of words \textit{w} with length from 0 to \textit{$L_2$} with the probabilities of words \textit{uw}, where \textit{u} is a prefix of \textit{w} with lengths from 0 to \textit{$L_1$}. This limits the number of tests to be realized.

A synchronization words is a good starting point to model a system from its output sequence because the probability of its occurrence does not depend on what comes before it. Therefore, its prefix can be regarded as a transient.

\subsection{Initial Partition for PFSA}\label{subsec:inipart}

In the current work, when applying a graph minimization algorithm (such as Moore or Hopcroft) on a PFSA graph, the following criterion is used to create the initial partition:

\begin{definition}\label{def:inipartpfsa}

Given a PFSA $\{G,\pi\}$, two states \textit{p, q} $\in Q$ are grouped together in an equivalence class if their morphs are equivalent via a statistical test, i.e., the null hypothesis $\mathcal{V}(p) = \mathcal{V}(q)$ is true for a confidence level $\alpha$.

\end{definition}



\section{Consolidated Algorithms}

In this section, two algorithms that construct a PFSA from a sequence $S$ of length $N$ over an alphabet $\Sigma$ are presented: D-Markov Machines and CRISSiS.

\subsection{D-Markov Machines}

A D-Markov machine is a PFSA that generates symbols that depend only on the history of at most $D$ previous symbols, in which $D$ is the machine's depth. It generates a Markov process $\{s_n\}$ of order $D$:

\[
\Pr(s_n|\ldots s_{n-D}\ldots s_{n-1}) = \Pr(s_n|s_{n-D}\ldots s_{n-1}).
\]

\noindent To construct a D-Markov Machine, first all symbol blocks of length \textit{D} are taken as the states in the set $Q$ and their transition probabilities can be computed as follows. Consider state $q$ labeled as $q = \sigma_1\sigma_2\ldots\sigma_{D}$ with $\sigma_n\in\Sigma\,$, for $n = 1,2,\ldots,D$. There is a transition from $q$ to $q^{\prime} = \sigma_2\ldots\sigma_{D}\tau$ for $\tau\in\Sigma$\,, that is $\delta(q,\tau) = q^{\prime}\,$, with probability:

\begin{equation}\label{eq:condprob}
\Pr(\tau | q) = \frac{\Pr(q\tau)}{\Pr(q)},
\end{equation}

\noindent where $\Pr(q)$ and $\Pr(q\tau)$ are the probabilities of $q$ and $q'$ occurring in the original sequence, respectively.

For example, considering a binary sequence \textit{S} with the probabilities of words of length $\ell \leq 3$ shown in Table \ref{tab:subseq}. To build a 2-Markov Machine, the states are 00, 01, 10 and 11.  Using Equation (\ref{eq:condprob}), the D-Markov machine shown in Figure \ref{fig:dmarkov} is built.

\begin{table}
\centering
\caption{Probabilities of words of length up to 3 obtained from a binary sequence \textit{S}. \label{tab:subseq}}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$\ell = 1$ & Prob. & $\ell = 2$ & Prob. & $\ell = 3$ & Prob. \\
\hline
0 & 0.51 & 00 & 0.27 & 000 & 0.15\\
1 & 0.49 & 01 & 0.23 & 001 & 0.12\\
  &      & 10 & 0.24 & 010 & 0.12\\
  &      & 11 & 0.25 & 011 & 0.11\\
  &	     &    &      & 100 & 0.12\\
  &      &    &      & 101 & 0.12\\
  &      &    &      & 110 & 0.11\\
  &      &    &      & 111 & 0.14\\
\hline
\end{tabular}
\end{table}

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =3 cm and 3cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (11)
{$11$};
\node[state] (01) [above left=of 11] {$01$};
\node[state] (10) [above right =of 11] {$10$};
\node[state] (00) [above right=of 01] {$00$};
\path (00) edge [loop above] node[above] {$0|0.56$} (00);
\path (00) edge [bend left = -15] node[above =0.35 cm] {$1|0.44$} (01);
\path (11) edge [loop below] node[below] {$1|0.56$} (11);
\path (11) edge [bend left = -15] node[below =0.35 cm] {$0|0.44$} (10);
\path (01) edge [bend left = 15] node[above =0.15 cm] {$0|0.5$} (10);
\path (01) edge [bend left = -15] node[above =0.35 cm] {$1|0.5$} (11);
\path (10) edge [bend right = 15] node[above =0.35 cm] {$0|0.5$} (00);
\path (10) edge [bend right = -15] node[below =0.15 cm] {$1|0.5$} (01);
\end{tikzpicture}
\caption{A D-Markov machine with sequence \textit{S} and $D$ = 2.\label{fig:dmarkov}}
\end{figure}
 
\subsection{CRISSiS}\label{sec:crissis}

The Compression via Recursive Identification of Self-Similar Semantics (CRISSiS) algorithm is presented in \citep{asok.11}. It assumes that a sequence \textit{S} over an alphabet $\Sigma$ of length \textit{N} which is generated by a synchronizable and irreducible PFSA. CRISSiS is shown in Algorithm \ref{alg:crissis} and it consists of three steps:

\begin{algorithm} 
  \caption{CRISSiS\label{alg:crissis}}
    \begin{algorithmic}[1]
      \State \textbf{Inputs:} Symbolic string $S, \Sigma, L_1, L_2$, significance level $\alpha$
      \State \textbf{Outputs:} PFSA \^{P} = \{$G, \pi$\}
      \State \textbf{\#\# Identification of Shortest Synchronization Word:}
      \State $\omega_{syn} \leftarrow $ null
      \State $d \leftarrow $ 0
      \While{$\omega_{syn}$ is null}
      	\State $\Omega \leftarrow \Sigma^d$
      	\ForAll{$\omega \in \Omega$}
      		\If{(isSynString($\omega, L_1, L_2$))}
      			\State $\omega_{syn}\leftarrow\omega$
      			\State \textbf{break}
      		\EndIf
      	\EndFor
      	\State $d \leftarrow d + 1$
      \EndWhile
      \State \textbf{\#\# Recursive Identification of States:}
      \State $Q \leftarrow$ \{$\omega_{syn}$\}
      \State  \~{Q} $ \leftarrow \{\omega_{syn}\sigma,\, \forall \sigma \in \Sigma\}$
      \State $\delta(\omega_{syn},\sigma) = \omega_{syn}\sigma  \forall \sigma\in\Sigma$
      \ForAll{$\omega\in $ \~{Q}}
      	\If{$\omega$ occurs in \textit{S}}
      		\State $\omega^* \leftarrow$ matchStates($\omega, Q, L_2$)
      		\If{$\omega^*$ is null}
      			\State Add $\omega$ to $Q$
      			\State Add $\omega\sigma$ to \~{Q} and $\delta(\omega,\sigma) = \omega_{syn}\sigma,\,  \forall \sigma\in\Sigma$
      		\Else
      			\State Replace all $\omega$ by $\omega^*$ in $\delta$
      		\EndIf
      	\EndIf
      \EndFor
      \State \textbf{\#\# Estimation of Morph Probabilities:}
      \State Find $k$ such that $S[k]$ is the symbol after the first occurrence of $\omega_{syn}$ in \textit{S}
      \State Initialize $\pi$ to zero
      \State \textit{state} $\leftarrow \omega_{syn}$
      \ForAll{i $\geq$ \textit{k} in \textit{S}}
		\State $\pi($\textit{state}, $S[i]) \leftarrow \pi($\textit{state},$S[i]) + 1$
		\State \textit{state} $\leftarrow \delta($\textit{state},$S[i])$      
      \EndFor 
      \State Normalize $\pi$ for each state
    \end{algorithmic}
  \end{algorithm}
  
\subsubsection{Identification of Shortest Synchronization Word}

Using the definition of a synchronization word given in (\ref{eq:practsynchword}), CRISSiS uses brute force to find the shortest synchronization word with fixed parameters $L_1$ and $L_2$. This is shown in Algorithm \ref{alg:issynstring} where each state morph is checked with the morph of its extensions up to a length $L_2$. If all statistical tests are positive for a given word $\omega$, it is returned as the synchronization word $\omega_{syn}$.

The auxiliar function \textit{hypothesisTest} is used to check (\ref{eq:practsynchword}) for a given value of $\alpha$. It can be implemented either as the $\chi^2$ test or the Kolmogorov-Smirnov test. It returns True when the test states that the probabilities are statistically the same or False when they are not.
  
\begin{algorithm}[b]
\caption{isSynString($\omega, L_1, L_2$)\label{alg:issynstring}}
	\begin{algorithmic}[1]
	\State \textbf{Outputs:} true or false
	\For{$D$ = 0 to $L_1$}
		\ForAll{$u \in \Sigma^D$}
			\For{$d$ = 0 to $L_2$}
				\ForAll{$v \in \Sigma^d$}
					\If{\textit{hypothesisTest}($\Pr(u|\omega v), \Pr(u|\omega v), \alpha) = $ False}
						\State \textbf{return} False
					\EndIf
				\EndFor
			\EndFor
		\EndFor
	\EndFor
	\State \textbf{return} true
	\end{algorithmic}
\end{algorithm}

\subsubsection{Recursive Identification of States}

States are equivalence class of strings under Nerode equivalence class. To check if two states $q_1$ and $q_2$ are equivalent, it would be necessary to test:

\begin{equation}\label{eq:classeqprob}
\Pr(v|q_1) = \Pr(v|q_2), \forall v \in \Sigma^*
\end{equation}

\noindent but as it is not feasible to test for strings up to infinite length, a simplified version checks for string up to length $L_2$:

% Given the synchronization word $\omega$, for any two strings $\omega_1$ and $\omega_2$ that reach a state $q$,
%
%\begin{equation}\label{eq:classeqprob}
%\Pr(\omega|\omega_1) = \Pr(\omega|\omega_2).
%\end{equation}
%
%These future conditional probabilities uniquely identify each state and Equation \ref{eq:classeqprob} can be used to check whether two states q$_1$ and q$_2$ are the same given $\omega_1\in$ q$_1$ and $\omega_1\in$ q$_2$. Once again, the problem of checking all possible strings can not be done in finite time, so only $L_2$-steps ahead are to be checked, giving:

\begin{equation}\label{eq:classeqprobd}
\Pr(v|q_1) = \Pr(v|q_2), \forall v \in \Sigma^d, d = 1,\ldots,L_2.
\end{equation}

\noindent If two states pass the statistical test using (\ref{eq:classeqprobd}), they are considered to be statistically the same. Strings $q_1$ and $q_2$ need to be synchronizing in order to use (\ref{eq:classeqprobd}). If $\omega$ is a synchronization word for some $q_i \in Q$, then $\omega\tau$ is also a synchronization word for $q_j = \delta(q_i,\tau)$.

%The next procedure starts by letting $Q$ be the set of states to be discovered for the PFSA and it is initialized containing only the state defined by the synchronization word $\omega_{syn}$ found in the first step, and \~{Q} is the final set of states recovered by the algorithm and it is initialized as an empty set.

 The next procedure starts by letting $Q$ be the set of states that will receive the states for the final machine found by the algorithm and \~{Q} is the set of states to be checked if they are equivalent to some state in $Q$ or if they are a state on their own. It is initialized with the descendants of $\omega_{syn}$. The function $\delta$ for the machine is initialized with $\delta(\omega_{syn},\sigma)$ equal to $\omega_{syn}\sigma$ for all $\sigma\in\Sigma$. This is represent by a tree using $\omega_{syn}$ as the root node to $|\Sigma|$ children, which are the states in \~{Q}. Each one of the children nodes is regarded as a candidate state. Each one of them is tested using a statistical test with confidence level $\alpha$ with each of the states in $Q$. If a match between some $\omega\in$ \~{Q} and some $\omega^*\in Q$ is found, the child state is removed and all the transitions to $\omega$ are redirected to $\omega^*$ (i.e. every $\delta(q,\sigma) = \omega$ now becomes $\delta(q,\sigma) = \omega^*$ for any $q\in Q$ and any $\sigma\in\Sigma$). If it does not match any state in $Q$, it is considered a new state and it is then added to $Q$ and it should also be split in $|\Sigma|$ new candidate states which are added to \~{Q}. This procedure is repeated until no new candidate states have to be visited. As CRISSiS should be applied to estimate a finite PFSA, this procedure is guaranteed to terminate. 

\begin{algorithm}[t]
\caption{matchStates($\omega, Q, L_2$)\label{alg:matchstates}}
\begin{algorithmic}[1]
	\ForAll{$q \in Q$ }
		\For{$d =$ 0 to $L_2$}
			\ForAll{$v \in \Sigma^d$}
				\If{\textit{hypothesisTest}($\Pr(\omega v|q), \Pr(v|q),\alpha$) = False}
					\State \textbf{return} $q$
				\EndIf
			\EndFor
		\EndFor
	\EndFor
	\State \textbf{return} null
\end{algorithmic}
\end{algorithm}

\subsubsection{Estimation of Morph Probabilities}

To recover the morphs of each state in $Q$ found in the last step, the sequence \textit{S} (starting from the first of occurrence of $\omega_{syn}$) is fed to the PFSA starting at state $\omega_{syn}$ and transition following the symbols of the original sequence. Each transition is counted and then normalized in order to recover an estimation of each state morph.

\subsubsection{Example}

The PFSA in Figure \ref{fig:trishift}, which is called Tri-Shift in this work, is presented in \citep{asok.11}. It is synchronyzable and works over a binary alphabet. It is used in this example to generate a string \textit{S} of length 10000. Table \ref{tab:trishiftsub} gives the estimated probabilities of subsequences occurring in \textit{S}. In this example, $L_1 = L_2 = 1$.

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =2 cm and 2 cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (A)
{$A$};
\node[state] (B) [above right=of A] {$B$};
\node[state] (C) [below right =of B] {$C$};
\path (A) edge [loop left] node[above] {$0|0.5$} (A);
\path (A) edge [bend left = 15] node[above =0.35 cm] {$1|0.5$} (B);
\path (B) edge [bend left = 15] node[above =0.35 cm] {$0|0.8$} (C);
\path (B) edge [bend left = 15] node[below =0.35 cm] {$1|0.2$} (A);
\path (C) edge [loop right] node[above] {$1|0.3$} (C);
\path (C) edge [bend left = 15] node[below =0.15 cm] {$0|0.7$} (A);
\end{tikzpicture}
\caption{The Tri-Shift PFSA.\label{fig:trishift}}
\end{figure}

\begin{table}
\centering
\caption{Probabilities of words generated by the Tri-Shift up to length 10000. \label{tab:trishiftsub}}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
$\ell = 1$ & Prob. & $\ell = 2$ & Prob. & $\ell = 3$ & Prob. & $\ell = 4$ & Prob. & $\ell \geq 5$ & Prob.\\
\hline
0 & 0.62711 & 00 & 0.35164 & 000 & 0.17565 & 0000 & 0.08673 & 00100 & 0.09881\\
1 & 0.37291 & 01 & 0.27546 & 001 & 0.17599 & 0001 & 0.08892 & 00101 & 0.04181\\
  &      & 10 & 0.27546 & 010 & 0.21451 & 0010 & 0.14062 & 001000 & 0.0499\\
  &      & 11 & 0.09745 & 011 & 0.06094 & 0011 & 0.03536 & 001001 & 0.04891\\
  &	     &    &      & 100 & 0.17599 & 0100 & 0.14206 & 001010 & 0.02926\\
  &      &    &      & 101 & 0.09946 & 0101 & 0.07245 & 001011 & 0.01255\\
  &      &    &      & 110 & 0.06094 & 1000 & 0.08892 &        &     \\
  &      &    &      & 111 & 0.03651 & 1001 & 0.08707 &        &     \\
  &      &    &      &     &      & 1100 & 0.03393 &        &     \\
  &      &    &      &     &      & 1101 & 0.02701 &        &     \\
\hline
\end{tabular}
\end{table}

First, the synchronization word needs to be found. States 0, 1 and so on are checked with (\ref{eq:practsynchword}). Starting by 0, $\Pr(0|0) = 0.5607$ is not equal to $\Pr(1|0) = \Pr(01)= 0.4393$ which means they do not pass the $\chi^2$ test. Then, the state 1 is tested, which also fails ($\Pr(0|1) = 0.7387 \neq 0.2613 = \Pr(1|1)$. For state 00, the probabilities are relatively close ($\Pr(0|00) = 0.5 = \Pr(1|00)$) and it passes the test, giving 00 the status of synchronization word.

The second step starts by defining the synchronization word state 00 adding it to $Q$ and splitting it into two candidates states, 000 and 001 (Figure \ref{fig:tree00}). Each candidate has its morphs compared to that of 00, which is the only state in $Q$, via (\ref{eq:classeqprobd}). $\mathcal{V}(000) = [0.494, 0.506]$ is considerably close to $\mathcal{V}(00) = [0.500, 0.500]$, so they pass the statistical test and 00 and 000 are considered to be an equivalent state. 000 is removed and the edge going from 00 to 000 becomes a self-loop from 00 to itself. On the other hand, $\mathcal{V}(001) = [0.800, 0.200]$ is considerably different from $\mathcal{V}(00)$, therefore it is considered a state and added to $Q$ (which now becomes \{00, 001\}) and then it is split into two new candidates (Figure \ref{fig:tree00-2}).

The same procedure is then repeated for the candidates 0010 and 0011. $\mathcal{V}(0010) = [0.703, 0.297]$ is different from both 00 and 001, therefore it is a new state, it is added to $Q$ and split into the new candidates 00100 and 00101. $\mathcal{V}(0011) = [0.500, 0.500]$ passes the test with $\mathcal{V}(00)$, which means that 0011 is removed and the edge from 001 to 0011 goes back to 00. This leads to the configuration in Figure \ref{fig:tree00-3}, with $Q$ = \{00, 001, 0010\}.

The next candidates are similar to two states in $Q$ ($\mathcal{V}(00100) = [0.505, 0.495]$ passes with 00 and $\mathcal{V}(00101) = [0.700, 0.300]$ passes with $\mathcal{V}(0010)$), so both are removed and its edges rearranged to the configuration in Figure \ref{fig:rectrishift}, which is the same graph as the original Tri-Shift, showing that CRISSiS recovered the PFSA graph. All that is left is to feed the input sequence to the graph and computing the morph probabilities, which recovers an accurate Tri-Shift PFSA.

\begin{figure}[t]
\centering
\begin{tikzpicture}
  [
    grow                    = right,
    sibling distance        = 6em,
    level distance          = 6em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\footnotesize},
    sloped
  ]
  \node [root] {00}
    child { node [env] {000}
      edge from parent node [below] {0} }
    child { node [env] {001}
      edge from parent node [above] {1} };
\end{tikzpicture}
\caption{Tree with 00 at its root, $Q$ = \{00\}. \label{fig:tree00}}
\end{figure}

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =2 cm and 2 cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (00)
{$00$};
\node[state] (001) [right=of 00] {$001$};
\node[state] (0010) [below right =of 001] {$0010$};
\node[state] (0011) [above right =of 001] {$0011$};
\path (00) edge [loop left] node[above =0.15 cm] {$0$} (00);
\path (00) edge node[above =0.15 cm] {$1$} (001);
\path (001) edge node[above =0.15 cm] {$1$} (0011);
\path (001) edge node[below =0.15 cm] {$0$} (0010);
\end{tikzpicture}
\caption{Second iteration of three, $Q$ = \{00, 001\}.\label{fig:tree00-2}}
\end{figure}

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =2 cm and 2 cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (00)
{$00$};
\node[state] (001) [right=of 00] {$001$};
\node[state] (0010) [right =of 001] {$0010$};
\node[state] (00100) [below right =of 0010] {$00100$};
\node[state] (00101) [above right =of 0010] {$00101$};
\path (00) edge [loop left] node[above =0.15 cm] {$0$} (00);
\path (00) edge [bend left = 15] node[above =0.15 cm] {$1$} (001);
\path (001) edge [bend left = 15] node[below =0.15 cm] {$1$} (00);
\path (001) edge node[below =0.15 cm] {$0$} (0010);
\path (0010) edge node[above =0.15 cm] {$0$} (00100);
\path (0010) edge node[below =0.15 cm] {$1$} (00101);
\end{tikzpicture}
\caption{Third iteration of the three, $Q$ = \{00, 001, 0010\}.\label{fig:tree00-3}}
\end{figure}

\begin{figure}
\centering
\begin {tikzpicture}[-latex ,auto ,node distance =2 cm and 2 cm ,on grid ,
semithick ,
state/.style ={ circle , draw = black , text=black , minimum width =1 cm}]
\node[state] (00)
{$00$};
\node[state] (001) [above right=of 00] {$001$};
\node[state] (0010) [below right =of 001] {$0010$};
\path (A) edge [loop left] node[above] {$0$} (A);
\path (A) edge [bend left = 15] node[above =0.35 cm] {$1$} (B);
\path (B) edge [bend left = 15] node[above =0.35 cm] {$0$} (C);
\path (B) edge [bend left = 15] node[below =0.35 cm] {$1$} (A);
\path (C) edge [loop right] node[above] {$1$} (C);
\path (C) edge [bend left = 15] node[below =0.15 cm] {$0$} (A);
\end{tikzpicture}
\caption{Recovered Tri-Shift topology.\label{fig:rectrishift}}
\end{figure}

\subsubsection{Time Complexity\label{crissiscomplex}}

As shown in \citep{asok.11}, CRISSiS operates with a time complexity of $O(N)\cdot(|\Sigma|^{O(|Q|^3)+L_1+L_2}+|Q||\Sigma|^{L_2})$, where $N$ is the length of the input sequence, $|\Sigma|$ is the alphabet size, $|Q|$ is the number of states in the original PFSA and $L_1$ and $L_2$ are parameters determining how much of the past and future of a state is needed to determine it. It is stated that as $L_1$ and $L_2$ are both usually small, it does not affect the performance greatly, even though the algorithm is exponential in these parameters.																					

%\subsection{Definitions}
%
%\begin{definition}[\textbf{Partitions and Equivalence Relations}]\label{def:partition}
%Given a set E, a partition of E is a family $\mathcal{P}$ of nonempty, pairwise disjoint subsets of E such that $\bigcup_{P\in\mathcal{P}}P = E $. The index of the partition is its number of elements. The partition defines an equivalence relation on E and the set of all equivalence classes [x], $x\in E$, of an equivalence relation in E defines a partition of the set.
%\end{definition}
%
%When a subset \textit{F} of \textit{E} is the union of classes of $\mathcal{P}$ it said that \textit{F} is saturated by $\mathcal{P}$. Given $\mathcal{Q}$, another partition of \textit{E}, it said to be a \textit{refinement} of $\mathcal{P}$ (or that $\mathcal{P}$ is coarser than $\mathcal{Q}$) if every class of $\mathcal{Q}$ is contained by some class of $\mathcal{P}$ and it is written as $\mathcal{Q} \leq \mathcal{P}$. The index of $\mathcal{Q}$ is greater than the index of $\mathcal{P}$.
%
%Given partitions $\mathcal{P}$ and $\mathcal{Q}$ of \textit{E}, $\mathcal{U} = \mathcal{P}\wedge\mathcal{Q}$ denotes the coarsest partition which refines $\mathcal{P}$ and $\mathcal{Q}$. The elements of $\mathcal{U}$ are non-empty sets \textit{P}$\cap$\textit{Q}, \textit{P}$\in\mathcal{P}$ and \textit{Q}$\in\mathcal{Q}$. The notation is extended for multiple sets as $\mathcal{U} = \mathcal{P}_1 \wedge \mathcal{P}_2 \wedge \ldots \wedge \mathcal{P}_n$. When $n=0$,  $\mathcal{P}$ is the universal partition comprised of just \textit{E} and it is the neutral element for the $\wedge$-operation.
%
%Given $F\subseteq E$, a partition $\mathcal{P}$ of \textit{E} induces a partition $\mathcal{P}'$  
%of \textit{F} by intersection. $\mathcal{P}'$ is composed by the sets $P\cap F$ with $P\subseteq \mathcal{P}$. If $\mathcal{P}$ and $\mathcal{Q}$ are partitions of \textit{E} and $\mathcal{Q} \leq \mathcal{P}$, the restrictions $\mathcal{P}'$ and $\mathcal{Q}'$ to \textit{F} maintain $\mathcal{Q}' \leq \mathcal{P}'$.
%
%Given partitions $\mathcal{P}$ and $\mathcal{P}'$ of disjoint sets \textit{E} and \textit{E'}, the partition of set $E \cup E'$ whose restriction to \textit{E} and \textit{E'} are $\mathcal{P}$ and $\mathcal{P'}$ is denoted by $\mathcal{P}\vee\mathcal{P}'$. It is possible to write $\mathcal{P} = \vee_{P\vee\mathcal{P}}\{P\}$.
%
%\begin{definition}[\textbf{Deterministic Automaton}]\label{def:minauto}
%A deterministic automaton over an alphabet $\Sigma$ is a quadruple $\mathcal{A}$ = (Q, i, $\delta$, F). Q is the set of states and $\delta$ is the transition function as in the PFSA from Definition \ref{definition:pfsa}. i $\in$ Q is the initial state from which the walk in this automaton will start. F $\subset$ Q is the set of final states in which the walks can end. For each q $\in$ Q, there is a corresponding subautomaton $\mathcal{A}$ starting in q, called the subautomaton rooted in q or simply automaton at q. 
%\end{definition}
%
%To each state \textit{q} there is a corresponding language $L_q(\mathcal{A}$), which is the set of sequences recognized by the automaton rooted at \textit{q}:
%
%\[
%L_q(\mathcal{A}) = \{w \in \Sigma* | qw \in F\}.
%\]


