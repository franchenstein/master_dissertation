\chapter{Introduction}\label{cap:intro}


{\lettrine[loversize=0.25,findent=0.2em,nindent=0em]{T}{he} current work presents a novel algorithm to model discrete dynamic systems using probabilistic finite state automata (PFSA) using techniques from graph minimization. 

Dynamic systems are of interest in a myriad of fields such as meteorology \cite{boulard1999characterization}, mechanics \cite{temam2012infinite} and neurobiology \cite{lewis2005bridging}. This systems often lead to chaos, which means that given two inputs close to each other (which can mean a difference smaller than $10^{-9}$ between them) produce outputs that greatly diverge from each other, making them difficult to predict and seem almost random even though being completely deterministic \cite{strogatz2001nonlinear}. A discrete dynamic system is a dynamic system which generates periodic outputs of symbols of a finite set (called an alphabet). It can also be obtained by periodically sampling the output of a dynamic system and mapping the samples into the symbols of an alphabet \citep{brianmarcus} \cite{rajagopalan2006symbolic}.

The goal of the algorithm developed in this work is to take the output of a discrete dynamic system and create a PFSA model of it by analyzing the statistics and structure inherent to the sequence. In order to obtain models that are less memory consuming, our algorithm applies techniques of graph minimization to obtain smaller PFSA.  The algorithm is first applied to sequences generated by other PFSA to determine how well it recovers the original systems and then it is applied to dynamic system sequences whose original system might not have a PFSA representation. The results are compared to other algorithms in the literature that seek similar goals. 

\section{Probabilistic Finite State Automata}

PFSA can be described as a finite graph whose nodes are called states and there are probabilities associated to each edge. Words or symbols generated by the PFSA are sequences of symbols from an alphabet obtained by going through a series of states respecting their edge probabilities. As in \cite{asok.11}, we consider the PFSA framework for which symbol generation is probabilistic and the end state is unique given a an initial state and a certain sequence. This differs from the framework presented in \cite{vidal.05} in which the symbol generation probabilities are not specified and there is a distribution over the possible end states.

In order to obtain models for dynamic systems and pattern classification, there are other methods and frameworks such as belief networks \cite{heckerman1995learning}, probabilistic context free grammars \cite{corazza2007probabilistic} and hidden Markov models \cite{rabiner1989tutorial}. The advantages of using PFSA are that they are simple and the sample time required for learning them is easy to characterize \citep{asok.11} and it is also an efficient framework for learning the causal structure of observed dynamical behavior \cite{murphy1995passively}. 

Other algorithms that construct PFSA include D-Markov machines \cite{ray2004symbolic}, which are Markov chains of a finite order $d$, meaning it uses the statistics of all subsequences of length $d$ to form its states; the Causal-State Splitting Reconstruction (CSSR) \cite{shalizi2004blind}, which starts by assuming that the process is an independent, identically-distributed (i.i.d.) sequence with one causal state and splits it to a probabilistic suffix tree of depth $L_{max}$. Each node on the tree defines a state labeled with a suffix and any two nodes are merged if the hypothesis that their next-symbol generation probability is the same according to some statistical test (such as $\chi^2$ or Kolmogorov-Smirnov) and then they are checked to keep them to be deterministic. There is also the Compression via Recursive Identification of Self-Similar Semantics (CRISSiS) \cite{asok.11} which firsts find a synchronization word in the sequence and use a state labeled with it as a starting point to construct the PFSA. It tests descendant states using statistical tests merging states if the test passes and creating new ones when it fails until an irreducible PFSA is obtained. As it has been shown in \citep{asok.11} that CRISSiS outperforms CSSR, the results obtained in this work are always compared to CRISSiS only.

For the current work, both the problem of obtaining the correct topology and edge probabilities has to be taken account. For this goal, given an input sequence the algorithm is broken down in a few steps: obtaining the probabilities of the subsequences and create a tree structure with each state representing a subsequence, finding the synchronization words, group the states in equivalence classes using a statistical criterion and applying a graph minimization algorithm to obtain an irreducible PFSA.

\section{Work Structure}

The work is presented as follows: chapter 2 reviews concepts from discrete sequences, PFSA and graph minimization algorithm while also showing the CRISSiS and D-Markov Machine algorithms. Chapter 3 then presents our ALEPH algorithm broken in each of its steps and then analyzes the time complexity of running it. Chapter 4 shows presents some dynamic systems that can be represented with PFSA and shows the comparative results of ALEPH, CRISSiS and D-Markov trying to retrieve the original PFSA. Chapter 5 shows applications for which it is not known if there is PFSA representation and how the three algorithms perform to model these systems. Finally, in chapter 6 a conclusion is discussed and plans for future works to improve the ALEPH algorithm are presented.


%%%%%%%%%%%%%%%%%%%%%% FIM DA SEÇÃO - Breve historia do caos %%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%% FIM DO CAPÍTULO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

